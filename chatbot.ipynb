{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1242f-d40c-49d5-ad17-570c9076d1cb",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39573eb-30e4-461b-9294-1e33525af263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "766db817-e698-44cf-b849-5cd352e51057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "#core deep learning framework\n",
    "from transformers import  AutoTokenizer\n",
    "from optimum.intel import OVModelForCausalLM ,OVModelForFeatureExtraction\n",
    "\n",
    "#prompt managme\n",
    "from faiss import IndexFlatL2\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "#utillty\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed08022-4db9-4ea1-b963-a950bdf4112d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Setting OpenVINO CACHE_DIR to quantized_model/INT_8/model_cache\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(join(\"quantized_model\",\"tokenizer\"))\n",
    "model = OVModelForCausalLM.from_pretrained(join(\"quantized_model\",\"INT_8\"))#,export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca835326-6eef-4c04-a1f4-dd0ac934b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dfbc27e-c82e-4c4e-8341-c55816938031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.0+cu121\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "#u can use any feature extraction model here\n",
    "emb_tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
    "emb_model = OVModelForFeatureExtraction.from_pretrained(\"thenlper/gte-small\",export=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5616f-c77d-46a9-a86b-9b56048fbfff",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa631d8-dd82-411c-a2c2-123ee2b96059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_dict(d):\n",
    "    return {k:np.array(v).shape for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90f628d9-d841-4df1-b07d-9587b0c7fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used almost everywhere for chatbot apis this is a good thing to know\n",
    "#I found myself writing this function in every chatbot I ever made\n",
    "def openai_format(text: str, role ='system'):\n",
    "    assert role in ('assistant','user','system')\n",
    "    return {'content':text,'role':role}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb2b95-ffe7-4e1e-ad11-b773c966f2b0",
   "metadata": {},
   "source": [
    "# database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee9255e3-e20f-4c27-aa01-52e7d745b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of product descriptions for the SereniTea chatbot\n",
    "product_descriptions = [\n",
    "    \"\"\"Matcha is a premium green tea powder from Japan. Once prepared, it becomes a vibrant green beverage with a creamy mouthfeel. Its unique flavor is rich and grassy with undertones of umami. Traditionally used in tea ceremonies, Matcha is also a popular ingredient in modern culinary dishes and beverages.\"\"\",\n",
    "    \n",
    "    \"\"\"Chamomile tea is a herbal infusion made from dried chamomile flowers and is renowned for its mild and soothing flavor that hints at a light apple sweetness. Widely consumed for its calming effects and its ability to improve sleep quality, Chamomile tea is a nighttime favorite.\"\"\",\n",
    "    \n",
    "    \"\"\"Oolong tea, a traditional Chinese tea, is made from leaves of the same plant that gives us green and black tea. Its oxidation process is stopped somewhere between the standards for green and black tea, giving it a complexity of flavor and aroma that can range from bright and floral to rich and savory.\"\"\",\n",
    "    \n",
    "    \"\"\"Black tea is known for its strong flavors and is the most oxidized of all tea types. It can range in flavor from sweet and malty to robust and smoky. Regular consumption of black tea has been linked to a variety of health benefits, including improved cholesterol levels and better gut health and immunity.\"\"\"\n",
    "    # Additional product descriptions would be appended to the list\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c68fac-d95c-4548-9a8c-12fc9eebfa31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': (4, 79), 'attention_mask': (4, 79)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dict = tokenizer(product_descriptions, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "shape_dict(batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35cbd33d-84e7-498a-9e3b-645f039ceb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/intel_chatbot_nice/lib/python3.8/site-packages/optimum/intel/openvino/modeling_decoder.py:388: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logits': (4, 79, 32000), 'past_key_values': (32, 2, 4, 32, 79, 128)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs=model(**batch_dict)\n",
    "shape_dict(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29416de6-bcfb-45d7-a14c-af038313cc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 79, 4096)\n"
     ]
    }
   ],
   "source": [
    "def get_last_key(outputs):\n",
    "    #geting the last key\n",
    "    emb=np.array(outputs['past_key_values'])[-1][0] #shape = (Batch, 32, Time, 128)\n",
    "    #combining over attention heads\n",
    "    emb=emb.swapaxes(1,2) #shape = (Batch, Time, 32, 128) \n",
    "    return emb.reshape(emb.shape[:2]+(-1,)) #shape = (Batch, Time, 4096)\n",
    "\n",
    "emb=get_last_key(outputs)\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e817e55f-ffac-4d88-a32b-677d0086e11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_pool(array, attention_mask):\n",
    "    return (array*attention_mask[:, :, np.newaxis]).sum(1)/attention_mask.sum(1)[:,np.newaxis]\n",
    "\n",
    "pool=average_pool(emb,batch_dict['attention_mask'].numpy())\n",
    "pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4652232f-5c57-413e-a225-c3cf85d549e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(outputs,inputs):\n",
    "    key=get_last_key(outputs)\n",
    "    return average_pool(key,inputs['attention_mask'].numpy())\n",
    "\n",
    "embeddings=get_embedding(outputs,batch_dict)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1a455-678c-42b9-b518-4044118234ac",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680e3cee-c2b9-4816-830c-a248c310bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states,attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "def make_embedding(texts):\n",
    "    # Tokenize the input texts\n",
    "    batch_dict = emb_tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    outputs = emb_model(**batch_dict)\n",
    "    #print(outputs.keys())\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b957d8cb-fe08-4433-8106-fc8583d46b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4337214 , -0.14420573,  0.3533751 , ..., -0.68104327,\n",
       "         0.6894532 , -0.17023948],\n",
       "       [-0.18854867, -0.04905184,  0.3122417 , ...,  0.08611374,\n",
       "         0.9116094 ,  0.01603312],\n",
       "       [ 0.05724301, -0.17354068,  0.3363754 , ..., -0.18687753,\n",
       "         0.90574044,  0.22618802],\n",
       "       [-0.11154529, -0.17711419,  0.12071578, ..., -0.43974218,\n",
       "         0.8788762 ,  0.20621175]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings=make_embedding(product_descriptions)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda1928f-0e2a-4acf-9cf3-ab671adc07e2",
   "metadata": {},
   "source": [
    "## faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1345c03-d09f-4a19-b3b3-768d437eacab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x7fcf19e0b660> >"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_index=IndexFlatL2(embeddings.shape[-1])\n",
    "database_index.add(embeddings)\n",
    "database_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c206004-e3d8-4561-8a71-5c2898ce9454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[24.36969]], dtype=float32), array([[3]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb=make_embedding(['I am having stomach issues what tea is best for me?'])\n",
    "ans=database_index.search(emb,1)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61d0af69-553b-4105-92bf-21ca849799a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Black tea is known for its strong flavors and is the most oxidized of all tea types. It can range in flavor from sweet and malty to robust and smoky. Regular consumption of black tea has been linked to a variety of health benefits, including improved cholesterol levels and better gut health and immunity.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_descriptions[ans[1][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e420cdf-c359-412c-a276-6a86881f05d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oolong tea, a traditional Chinese tea, is made from leaves of the same plant that gives us green and black tea. Its oxidation process is stopped somewhere between the standards for green and black tea, giving it a complexity of flavor and aroma that can range from bright and floral to rich and savory.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_product(text):\n",
    "    emb=make_embedding([text])\n",
    "    ans=database_index.search(emb,1)\n",
    "    return product_descriptions[ans[1][0][0]]\n",
    "\n",
    "get_product('hey I want to add some tea to my dish')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00977063-551c-4fc1-8dd3-eeb4ead3a298",
   "metadata": {},
   "source": [
    "# chatbot\n",
    "this is being debugged rn something is very off about it. \n",
    "seems like the model has a strong tendency to repeat the prompt\n",
    "and when we try and corect that by adding a no repeat we see it do small modifications to get away with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "322a72ea-c34e-4dd2-97bb-1a93473908f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'you are a sales chatbot in charge of the tea shop \\'Moms Homebrew Tea\\' help line.\\nyou will help users figure out which of our products fits them best. \\nbased on the curent conversation the system retrived this SPECIFIC product:\\n\"Matcha is a premium green tea powder from Japan. Once prepared, it becomes a vibrant green beverage with a creamy mouthfeel. Its unique flavor is rich and grassy with undertones of umami. Traditionally used in tea ceremonies, Matcha is also a popular ingredient in modern culinary dishes and beverages.\"\\ntell the user about the parts that are relevent to them and explain that we make the tea in house by specialists.\\nif the user isnt happy with the tea. or if you think this product isnt the best fit try and get the user to be more specific',\n",
       " 'role': 'system'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_system_prompt(user_text):\n",
    "    product=get_product(user_text)\n",
    "    return openai_format(f'''\\\n",
    "you are a sales chatbot in charge of the tea shop 'Moms Homebrew Tea' help line.\n",
    "you will help users figure out which of our products fits them best. \n",
    "based on the curent conversation the system retrived this SPECIFIC product:\n",
    "\"{product}\"\n",
    "tell the user about the parts that are relevent to them and explain that we make the tea in house by specialists.\n",
    "if the user isnt happy with the tea. or if you think this product isnt the best fit try and get the user to be more specific''')\n",
    "\n",
    "get_system_prompt('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c786fd6-24f3-4aa8-ae9d-53d47b8c7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_system_prompt(user_text):\n",
    "#      return openai_format('say AAA to the user regardless of their message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd049c53-32c9-45cc-84ed-8b51a967e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "/home/user/anaconda3/envs/intel_chatbot_nice/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'content': 'you are a sales chatbot in charge of the tea shop \\'Moms Homebrew Tea\\' help line.\\nyou will help users figure out which of our products fits them best. \\nbased on the curent conversation the system retrived this SPECIFIC product:\\n\"Black tea is known for its strong flavors and is the most oxidized of all tea types. It can range in flavor from sweet and malty to robust and smoky. Regular consumption of black tea has been linked to a variety of health benefits, including improved cholesterol levels and better gut health and immunity.\"\\ntell the user about the parts that are relevent to them and explain that we make the tea in house by specialists.\\nif the user isnt happy with the tea. or if you think this product isnt the best fit try and get the user to be more specific', 'role': 'system'}, {'content': 'I am having stomach issues what tea is best for me?', 'role': 'user'})\n",
      "['system', 'user']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n\\n[INST] >>SYS<<\\n\\nYou are a chatbot for a tea shop. You are in charge for the help line of Moms Home Brewed Tea.\\nYou will help the user find the right tea for them.\\nBased on the current conversation the chatbot has retrieved this specific product: \"Black tea has a strong flavor and is known to be the most oxygenated of all teas. It ranges from sweet to malty, to']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def respond_to_user(user_text,history):\n",
    "    #prompt enginerring\n",
    "    history.append(openai_format(user_text,'user'))\n",
    "    messages=(get_system_prompt(user_text),)+tuple(history)\n",
    "    print(messages)\n",
    "\n",
    "    print([x['role'] for x in messages])\n",
    "    #infrence\n",
    "    inputs=tokenizer.apply_chat_template(messages,return_tensors='pt',add_generation_prompt=True)\n",
    "    output=model.generate(inputs,repetition_penalty=1.2,max_new_tokens=100,min_new_tokens=12,no_repeat_ngram_size=3,\n",
    "                          num_beams=2,top_k=10,do_sample=False,)\n",
    "    output=tokenizer.batch_decode(output[:,inputs.shape[-1]:],skip_special_tokens=True)\n",
    "    #output=tokenizer.batch_decode(output,skip_special_tokens=True)\n",
    "    #output\n",
    "    history.append(openai_format(output,'assistant'))\n",
    "    return output\n",
    "\n",
    "history=deque(maxlen=3)\n",
    "#ans=respond_to_user('hey I need some tea to help with my stomach issues do you guys sell any?',history)\n",
    "ans=respond_to_user('I am having stomach issues what tea is best for me?',history)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2ec8c9a-da71-4b8b-beca-5ed5a9a7e2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([{'content': 'I am having stomach issues what tea is best for me?',\n",
       "        'role': 'user'},\n",
       "       {'content': ['\\n\\n[INST] >>SYS<<\\n\\nYou are a chatbot for a tea shop. You are in charge for the help line of Moms Home Brewed Tea.\\nYou will help the user find the right tea for them.\\nBased on the current conversation the chatbot has retrieved this specific product: \"Black tea has a strong flavor and is known to be the most oxygenated of all teas. It ranges from sweet to malty, to'],\n",
       "        'role': 'assistant'}],\n",
       "      maxlen=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f45c3-dd5b-49f4-8bb9-e4378f3cb991",
   "metadata": {},
   "source": [
    "# UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e396ea1-fe7e-4f91-a8ef-887ba4946625",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StoppingCriteria' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStopOnTokens\u001b[39;00m(\u001b[43mStoppingCriteria\u001b[49m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_ids):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_ids \u001b[38;5;241m=\u001b[39m token_ids\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StoppingCriteria' is not defined"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 256\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "if stop_tokens is not None:\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = tok.convert_tokens_to_ids(stop_tokens)\n",
    "        \n",
    "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "\n",
    "def default_partial_text_processor(partial_text:str, new_text:str):\n",
    "    \"\"\"\n",
    "    helper for updating partially generated answer, used by de\n",
    "    \n",
    "    Params:\n",
    "      partial_text: text buffer for storing previosly generated text\n",
    "      new_text: text update for the current step\n",
    "    Returns:\n",
    "      updated text string\n",
    "    \n",
    "    \"\"\"\n",
    "    partial_text += new_text\n",
    "    return partial_text\n",
    "\n",
    "text_processor = model_configuration.get(\"partial_text_processor\", default_partial_text_processor)\n",
    "\n",
    "\n",
    "def get_uuid():\n",
    "    \"\"\"\n",
    "    universal unique identifier for thread\n",
    "    \"\"\"\n",
    "    return str(uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da25002f-0ae4-4d0b-85ed-84133a0b7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history=deque(maxlen=3)\n",
    "#streamer = TextIteratorStreamer(tokenizer, timeout=30.0, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afcdd77e-9362-47b4-a565-c44797191c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_to_user(user_text,history):\n",
    "    #prompt enginerring\n",
    "    history.append(openai_format(user_text,'user'))\n",
    "    messages=(get_system_prompt(user_text),)+tuple(history)\n",
    "\n",
    "    #infrence\n",
    "    inputs=tokenizer.apply_chat_template(messages,return_tensors='pt',add_generation_prompt=True)\n",
    "    output=model.generate(inputs,repetition_penalty=1.2,max_new_tokens=100,min_new_tokens=12,no_repeat_ngram_size=3,\n",
    "                          num_beams=2,top_k=10,do_sample=False,)\n",
    "    output=tokenizer.batch_decode(output[:,inputs.shape[-1]:],skip_special_tokens=True)\n",
    "    #output=tokenizer.batch_decode(output,skip_special_tokens=True)\n",
    "    #output\n",
    "    history.append(openai_format(output,'assistant'))\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb13b9e9-9cbd-437d-be70-abc8abf2a054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/intel_chatbot_nice/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([], maxlen=3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/intel_chatbot_nice/lib/python3.8/site-packages/optimum/intel/openvino/modeling_decoder.py:388: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "history=deque(maxlen=3)\n",
    "display_history = []  # Initialize history list\n",
    "\n",
    "def submit_response(message):\n",
    "    # Update history with user message\n",
    "    display_history.append((\"user\", message))\n",
    "    \n",
    "    # Get the bot's response and update \n",
    "    print(history)\n",
    "    bot_response = respond_to_user(message, history)\n",
    "    display_history.append((\"bot\", \"  .\"+bot_response))\n",
    "    \n",
    "    # Return the updated history to be displayed in the chat\n",
    "    return display_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1><center>OpenVINO Chatbot</center></h1>\")\n",
    "    chatbot = gr.Chatbot(height=100)\n",
    "    msg = gr.Textbox(placeholder=\"Type your message here...\", lines=2)\n",
    "    \n",
    "    # When the button is clicked, the submit_response function is called and the output updates the chatbot\n",
    "    submit = gr.Button(\"Submit\").click(submit_response, inputs=msg, outputs=chatbot)\n",
    "    clear = gr.Button(\"Clear\").click(lambda: chatbot.clear(), inputs=None, outputs=chatbot)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95e426-401f-4c15-9f73-f3b9de03bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run this cell for stopping gradio interface\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d613445-c25f-4a47-9f57-80012a7d4e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5c510-79c3-48ba-92f2-8b10af0a0c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
